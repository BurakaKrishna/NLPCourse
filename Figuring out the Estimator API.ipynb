{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal\n",
    "This notebook is my live notes for figuring out how to use Tensorflows  Estimator API. \n",
    "I read the [official docs](https://www.tensorflow.org/guide/estimators) but they were a bit vauge for my taste, e.g. I couldn't read them and know what to do. \n",
    "\n",
    "For context, I have a toy problem, that has numbers as words in German and the actual words. I want to do two toy problems, one is classify whether a given word is even or odd. The other is convert from word to number using [NALU](https://arxiv.org/abs/1808.00508). It's 20:30 now and I have a baby that's switching between crying, feeding and sleeping. Let's see how far he lets us get. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step one - \"Literature Review\"\n",
    "The official docs point to a collection of [official models](https://github.com/tensorflow/models/tree/master/official) that are well maintained and serve as references for the high level APIs like Estimator. This is great. Even better, they have an implementation of the [Transformer model](https://github.com/tensorflow/models/tree/master/official/transformer) which is in the realm of NLP. That;s articulrly important because dataloading in NLP is a black art and getting it to play nice with a new API will be blacker than black so it's nice to have a reference to copy paste from\n",
    "![funny](https://i.gr-assets.com/images/S/compressed.photo.goodreads.com/books/1457364208i/29437996._UY630_SR1200,630_.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Caveat - The Transformer implementation is too good for me\n",
    "The implementation in the transormer model is a bit too good. It covers a lot of things I don't really need now like Bleu scores, running on TPUs and distributed training. Yes, these are highlights of the Estimator API but they are highlights I don't need now. So really the first thing I'll do is mark what I need to keep and delete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What we know\n",
    "\n",
    "So basically the Estimator API says \"Give me a function that returns Data, and another function that returns a model, and I'll combine them, run them, calculate the metrics, save checkpoints, distribute it across nodes and make you coffee. \n",
    "\n",
    "\n",
    "So what we need to figure out is \n",
    "* How to write a model function\n",
    "* What are the specifications for the data function\n",
    "    * Can we use feed dicts or only TFRecords ? \n",
    "    * It looks like we need to seperate examples and labels for the API, where should we do that ? \n",
    "    * Can we preproccess the text in Python. \n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to write a model function \n",
    "So the transformer model function is [here](https://github.com/tensorflow/models/blob/master/official/transformer/transformer_main.py#L69)\n",
    "Notably, it's signature is \n",
    "```python\n",
    "def model_fn(features, labels, mode, params):\n",
    "\n",
    "```\n",
    "I guess features is inputs, labels is labels, I know that mode is one of [Train,Test,Predict] or something semantically equaivalent. \n",
    "If you look [here]((https://github.com/tensorflow/models/blob/master/official/transformer/transformer_main.py#L81-L89) you'll see that the model returns something in the case that mode==PREDICT and then [here]((https://github.com/tensorflow/models/blob/master/official/transformer/transformer_main.py#L125-L138) something else if mode==TRAIN.\n",
    "\n",
    "Cool. so first thing we know is that it should do different things based on mode. This is actually super duper awesome because before this their wasn't a canonical way to make that seperation and every project reinvented the wheel. So while a bit complex, it's great. \n",
    "\n",
    "### What is the model_fn returning\n",
    "So when we look at what the model function is returing we see it returns an [EstimatorSpec](https://www.tensorflow.org/api_docs/python/tf/estimator/EstimatorSpec)  \n",
    "```python\n",
    "      return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "\n",
    "```\n",
    "Just looking at how it's called here this thing actually makes sense. \n",
    "The docs say\n",
    "\n",
    ">For mode == ModeKeys.TRAIN: required fields are loss and train_op.\n",
    "\n",
    ">For mode == ModeKeys.EVAL: required field is loss.\n",
    "\n",
    ">For mode == ModeKeys.PREDICT: required fields are predictions.\n",
    "\n",
    "\n",
    "So that tells us what the minumum we need to pass in. I wonder why we need to pass the loss into the estimator during training, since I assume it's implied in the training op that is minimizing the loss. But whatever.\n",
    "\n",
    "### Where did loss and train_op come from ?\n",
    "\n",
    "So we saw in that little python snippet that we pass loss and train_op but where did they come from ?\n",
    "So in the begining of the model_fn they do \n",
    "```python\n",
    "    model = transformer.Transformer(params, mode == tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "    logits = model(inputs, targets)\n",
    "```\n",
    "Where transformer is an import from some other directory. \n",
    "Then, once they've checked they are not in prediction mode, they calculate the loss\n",
    "```python\n",
    "    xentropy, weights = metrics.padded_cross_entropy_loss(\n",
    "        logits, targets, params[\"label_smoothing\"], params[\"vocab_size\"])\n",
    "    loss = tf.reduce_sum(xentropy) / tf.reduce_sum(weights)\n",
    "```\n",
    "Then they check if they are in eval or train mode, and if they are in train mode they also set up the train op \n",
    "```python\n",
    "      train_op, metric_dict = get_train_op_and_metrics(loss, params)\n",
    "```\n",
    "Where get_train_op_and_metrics is defined in the file. \n",
    "\n",
    "### Two patterns emerge\n",
    "The first pattern to emerge is that they calculate only what is needed. Instead of saying calculate we can say they only set up the graph-ops that are needed. E.g. they don't make the train_op if they don't need it. I wonder if this is just good engineering or serves a more \"practical purpose\", e.g. to leave space on GPU or someething. \n",
    "\n",
    "Second pattern to emerge, which is **more important** is their use of imports and helper functions. Obviously this is a good pratice, and I mention it because i feel that when I copy and paste from X I'm not always confident about what \"style\" of programming I can use. This holds especially true in ML and frontend code where the software engineering chops vary wildly within the respective communities. But I digress\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Who calls the model_fn ?\n",
    "So we've seen that the model_fn they define returns an esitmator spec when it gets data and a mode. Who calls it the model_fn and where does it get the data from ? \n",
    "Well [here]((https://github.com/tensorflow/models/blob/master/official/transformer/transformer_main.py#L499-L502) is an example. Let's copy paste it!\n",
    "\n",
    "```python\n",
    "    return tf.estimator.Estimator(\n",
    "        model_fn=model_fn, model_dir=flags_obj.model_dir, params=params,\n",
    "        config=tf.estimator.RunConfig(train_distribute=distribution_strategy))\n",
    "```\n",
    "\n",
    "Cool. So it makes sense that the Estimator class will call the model function, which will in turn return an estimator spec based on the mode. The thing is, model_fn is called with features and labels, where did they come from ? It's not obvious just from looking at this function. I can only assume that it's somehow specified in params. I vaugely remembered something about an input_fn so I searched the code for it and found \n",
    "[this morsel]((https://github.com/tensorflow/models/blob/master/official/transformer/transformer_main.py#L334-L337) \n",
    "```python\n",
    "    estimator.train(\n",
    "        dataset.train_input_fn,\n",
    "        steps=schedule_manager.single_iteration_train_steps,\n",
    "        hooks=train_hooks)\n",
    "```\n",
    "And so, I realize that the estimator instance returned from instantiaing the Estimator class with our model_fn has a method on it called train that accepts an input function. Shoutout to timeless [Execution in the Kingdom of nouns](https://steve-yegge.blogspot.com/2006/03/execution-in-kingdom-of-nouns.html).\n",
    "It's actually very sensible, an instance of an estimator has a bunch of methods, like train, evaluate and predict, which happen to correspond to the things we'd like to do with a model. In any of these cases, we need to provide data to our model, which is done through an input_fn. We can do a bunch of extra fancy things which we'll get to later. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A question is answered! We now know the constraints on the data input\n",
    "We can now go look at the docs for the estimators [train](https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator#train) method and see what we can pass around in the input_fn. To quote the docs\n",
    "\n",
    "        input_fn: A function that provides input data for training as minibatches. See Premade Estimators for more information. The function should construct and return one of the following: \n",
    "        * A tf.data.Dataset object: Outputs of Dataset object must be a tuple (features, labels) with same constraints as below. \n",
    "        * A tuple (features, labels): Where features is a tf.Tensor or a dictionary of string feature name to Tensor and labels is a Tensor or a dictionary of string label name to Tensor. \n",
    "        Both features and labels are consumed by model_fn. They should satisfy the expectation of model_fn from inputs.\n",
    "\n",
    "When we set out on this adventure I asked \n",
    "* Can we use feed dicts or only TFRecords ? \n",
    "* It looks like we need to seperate examples and labels for the API, where should we do that ? \n",
    "* Can we preproccess the text in Python. \n",
    "\n",
    "Let's answer them\n",
    "\n",
    "### Can we use feed dicts or only TFRecords ? \n",
    "Apperently no way to use feeddicts. But, we don't need to use TFRecords, we just need a function that reads data on the fly and returns tensors.\n",
    "### It looks like we need to seperate examples and labels for the API, where should we do that ? \n",
    "Well, input function says to return a Dataset object that returns a tuple. It isn't very specific about what that tuple should be. \n",
    "\n",
    "I'm not being nitpicky, I like to pass a lot of tensors in dictionaries, for example, I want to pass my examples in a tensor and another tensor with their lengths and put both of those in a dict. Apperently, I can, so long as I return a tuple of two dicts.\n",
    "\n",
    "I'll guestimate that the tensorflow folks were trying to avoid stuff like this when desiging the API. Probably the logic is that if you follow it to the letter you'll have super portable models that you can share and swap out parts. In my way, with the dicts, your model_fn now needs to parse dicts. Works for me. \n",
    "\n",
    "### Can we preproccess the text in Python ?\n",
    "I think we can. We'll try to do that in a bit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up for Tensorboard\n",
    "Let's face it, the best part of doing deep learning is watching the loss go down on Tensorboard. While the estimator API promises to let us do that for free, we haven't seen how. \n",
    "\n",
    "So actually, in the Transformer example they have this cool function that gets a loss and some params and returns both the train ops and the metrics we want in tensorflow. It's [here](https://github.com/tensorflow/models/blob/master/official/transformer/transformer_main.py#L166-L194). Basically they have a dict called metric_dict and it has names of scalars and scalrs. Then they run a function, [record_scalars](https://github.com/tensorflow/models/blob/master/official/transformer/transformer_main.py#L141) and that sets up the scalars for tensorboard. \n",
    "\n",
    "If you dig around, they do the same thing in slightly different ways, but really their is no magic here. You call tf.summary.X (or tf.contrib.summary.X)  and Estimator will take care of the rest. Amen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Summary - Setting up some context\n",
    "As I mentioned, I have a toy task! It consists of taking a word in German that represents a number and predicting if it is even or odd. Conveniently, I have a program that gives me dicts whose keys are numbers and values are  their German word equivalent. Check it out (Disclaimer, I wrote this program and my German is shameful so maybe its wrong) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{8: 'acht',\n",
       " 11: 'elf',\n",
       " 14: 'vierzehn',\n",
       " 20: 'undzwanzig',\n",
       " 54: 'vierundfünfzig',\n",
       " 62: 'zweiundsechzig',\n",
       " 63: 'dreiundsechzig',\n",
       " 72: 'zweiundsiebzig',\n",
       " 82: 'zweiundachtzig',\n",
       " 91: 'einundneunzig'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.numtoWord import createNum2WordDict\n",
    "createNum2WordDict(size=10,high=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool! \n",
    "Now, let's see how we make that into something that maps Words to even numbers. (0 means odd, 1 means even)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: ('eins', 0),\n",
       " 3: ('drei', 0),\n",
       " 44: ('vierundvierzig', 1),\n",
       " 52: ('zweiundfünfzig', 1),\n",
       " 59: ('neunundfünfzig', 0),\n",
       " 69: ('neunundsechzig', 0),\n",
       " 71: ('einundsiebzig', 0),\n",
       " 79: ('neunundsiebzig', 0),\n",
       " 90: ('undneunzig', 1),\n",
       " 93: ('dreiundneunzig', 0)}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = createNum2WordDict(size=10,high=100)\n",
    "d = {key: (val,(key+1)%2) for key,val in d.items()}\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, so now that I have my data here is what I want\n",
    "1. A model that \n",
    "    * Is an LSTM\n",
    "    * Reads the words charechter by charechter\n",
    "    * Predicts if they are even or odd\n",
    "2. An input function that \n",
    "    * calls my fancy function above and returns tensors in the proper format\n",
    "3. An Estimarot that\n",
    "    * Uses my model via a model_fn and my input_fn to train and evaluate \n",
    "    * To see progress and accuracy in Tensorboard\n",
    "    * As a bonus, to do a one line deploy of this to Google-ml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "So now I can summarize what I've learnt in light of what I want to do, e.g. derive a recipe. \n",
    "Basically\n",
    "1. Find your data\n",
    "2. Write a function that returns a Dataset object which in itself returns a tuple\n",
    "3. Define your model somewhere, as a function that returns logits / predictions\n",
    "4. Write a model_fn, \n",
    "    * Takes as input\n",
    "        * features and labels are the tuple from your input function\n",
    "        * mode is one of the values of [ModeKeys](https://www.tensorflow.org/api_docs/python/tf/estimator/ModeKeys)\n",
    "        * params are paramaters we haven't disucssed \n",
    "    * Returns an instance of an EstimatorSpec\n",
    "        * That does what needs to be done based on the mode (e.g. trains, or just predicts) \n",
    "5. Instantiate an Estimator with the model_fn\n",
    "6. Call estimator.train/eval/predict with the relevant input_fn\n",
    "\n",
    "It is now 22:07, so it took me an hour and forty to figure this out. My child did not interfere much so this was more or less continuous. \n",
    "\n",
    "Armed with this new knoweledge, I'm going to walk the dogs and then actually do this\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
